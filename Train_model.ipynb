{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 22:54:46.446849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target resolution: 224x224\n"
     ]
    }
   ],
   "source": [
    "# Spine Classification Training Notebook\n",
    "# This notebook demonstrates how to train a deep learning model for classifying spine images into two categories: **NormalFinal** and **ScolFinal**.\n",
    "\n",
    "# 1. Setting up \n",
    "\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Parameters\n",
    "target_height = 224\n",
    "target_width = 224\n",
    "batch_size = 16\n",
    "val_split = 0.1\n",
    "random_seed = 123\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "print(f\"Target resolution: {target_height}x{target_width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loading\n",
    "\n",
    "# Base directory containing the training images\n",
    "base_dir = \"Spines/CombinedTrainImages\"\n",
    "classes = [\"NormalFinal\", \"ScolFinal\"]\n",
    "\n",
    "def get_file_list(class_dir):\n",
    "    folder = os.path.join(base_dir, class_dir)\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) \n",
    "            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))]\n",
    "\n",
    "# Create a dictionary with file lists for each class\n",
    "files_dict = {c: get_file_list(c) for c in classes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Creating Training and Validation Datasets\n",
    "\n",
    "#In this section, we split the data into training and validation sets. For each class, we shuffle the file list and apply a fixed validation split.\n",
    "\n",
    "train_files, train_labels = [], []\n",
    "val_files, val_labels = [], []\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    files = files_dict[class_name]\n",
    "    random.shuffle(files)\n",
    "    split_idx = int(len(files) * val_split)\n",
    "    \n",
    "    # Validation data\n",
    "    val_files.extend(files[:split_idx])\n",
    "    val_labels.extend([class_idx] * split_idx)\n",
    "    # Training data\n",
    "    train_files.extend(files[split_idx:])\n",
    "    train_labels.extend([class_idx] * (len(files) - split_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Augmentation and Preprocessing\n",
    "\n",
    "# This section defines functions for data augmentation and preprocessing:\n",
    "# augment_image: Applies random flips, brightness/contrast adjustments, and random cropping.\n",
    "# preprocess: Reads, decodes, resizes the image, and applies MobileNetV2 preprocessing.\n",
    "\n",
    "def augment_image(image, label):\n",
    "    # Apply random flips\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    # Apply random brightness and contrast adjustments\n",
    "    image = tf.image.random_brightness(image, 0.2)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    \n",
    "    # Random crop to 200x200 and resize back to target dimensions\n",
    "    image = tf.image.random_crop(image, size=[200, 200, 3])\n",
    "    image = tf.image.resize(image, [target_height, target_width])\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def preprocess(file_path, label):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "    image = tf.image.resize(image, [target_height, target_width])\n",
    "    image = preprocess_input(image)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Build TensorFlow Datasets\n",
    "\n",
    "#We create TensorFlow datasets for both training and validation. The training dataset is shuffled and augmented, while the validation dataset is only preprocessed.#\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
    "train_ds = train_ds.shuffle(1000).map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels))\n",
    "val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Phase 1 (Frozen Base) ---\n",
      "Epoch 1/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 987ms/step - accuracy: 0.6760 - loss: 0.5924 - val_accuracy: 0.7143 - val_loss: 0.7274\n",
      "Epoch 2/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 907ms/step - accuracy: 0.8688 - loss: 0.2929 - val_accuracy: 0.7857 - val_loss: 0.6003\n",
      "Epoch 3/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 793ms/step - accuracy: 0.9220 - loss: 0.1747 - val_accuracy: 0.7857 - val_loss: 0.5905\n",
      "Epoch 4/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 820ms/step - accuracy: 0.9368 - loss: 0.1733 - val_accuracy: 0.8214 - val_loss: 0.5888\n",
      "Epoch 5/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 796ms/step - accuracy: 0.9545 - loss: 0.1558 - val_accuracy: 0.8214 - val_loss: 0.5920\n",
      "Epoch 6/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9465 - loss: 0.1592 - val_accuracy: 0.8929 - val_loss: 0.4665\n",
      "Epoch 7/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 894ms/step - accuracy: 0.9379 - loss: 0.1447 - val_accuracy: 0.8214 - val_loss: 0.6050\n",
      "Epoch 8/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 827ms/step - accuracy: 0.9730 - loss: 0.0920 - val_accuracy: 0.8214 - val_loss: 0.6319\n",
      "Epoch 9/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 870ms/step - accuracy: 0.9706 - loss: 0.1024 - val_accuracy: 0.8571 - val_loss: 0.6019\n",
      "Epoch 10/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9648 - loss: 0.0867 - val_accuracy: 0.8929 - val_loss: 0.5286\n",
      "Epoch 11/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9582 - loss: 0.1311 - val_accuracy: 0.8571 - val_loss: 0.6052\n",
      "Epoch 12/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.9771 - loss: 0.0730 - val_accuracy: 0.8929 - val_loss: 0.5621\n",
      "Epoch 13/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9623 - loss: 0.1122 - val_accuracy: 0.8571 - val_loss: 0.6087\n",
      "Epoch 14/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9682 - loss: 0.0976 - val_accuracy: 0.8929 - val_loss: 0.5448\n",
      "Epoch 15/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - accuracy: 0.9874 - loss: 0.0620 - val_accuracy: 0.8571 - val_loss: 0.6254\n",
      "Epoch 16/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 983ms/step - accuracy: 0.9788 - loss: 0.0690 - val_accuracy: 0.8929 - val_loss: 0.5129\n",
      "Epoch 17/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.9751 - loss: 0.0834 - val_accuracy: 0.8929 - val_loss: 0.4835\n",
      "Epoch 18/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 985ms/step - accuracy: 0.9854 - loss: 0.0636 - val_accuracy: 0.8929 - val_loss: 0.4724\n",
      "Epoch 19/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 992ms/step - accuracy: 0.9550 - loss: 0.0984 - val_accuracy: 0.8929 - val_loss: 0.5252\n",
      "Epoch 20/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9823 - loss: 0.0531 - val_accuracy: 0.8929 - val_loss: 0.5079\n",
      "\n",
      "--- Training Phase 2 (Fine-Tuning) ---\n",
      "Epoch 1/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.6604 - loss: 0.8163 - val_accuracy: 0.8929 - val_loss: 0.4417\n",
      "Epoch 2/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.7985 - loss: 0.5539 - val_accuracy: 0.8929 - val_loss: 0.4182\n",
      "Epoch 3/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.8131 - loss: 0.4253 - val_accuracy: 0.8929 - val_loss: 0.4010\n",
      "Epoch 4/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8689 - loss: 0.3012 - val_accuracy: 0.8929 - val_loss: 0.4021\n",
      "Epoch 5/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8820 - loss: 0.3460 - val_accuracy: 0.8929 - val_loss: 0.4085\n",
      "Epoch 6/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9114 - loss: 0.1970 - val_accuracy: 0.8929 - val_loss: 0.4159\n",
      "Epoch 7/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8984 - loss: 0.2095 - val_accuracy: 0.8929 - val_loss: 0.4130\n",
      "Epoch 8/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9371 - loss: 0.1433 - val_accuracy: 0.8929 - val_loss: 0.4041\n",
      "Epoch 9/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9428 - loss: 0.1201 - val_accuracy: 0.8929 - val_loss: 0.3994\n",
      "Epoch 10/10\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.9432 - loss: 0.1198 - val_accuracy: 0.8929 - val_loss: 0.4053\n"
     ]
    }
   ],
   "source": [
    "# 6. Building and Training the Model\n",
    "\n",
    "#We use MobileNetV2 as our base model (initialized with ImageNet weights). The training is done in two phases:\n",
    "#1. Phase 1 (Frozen Base): Only train the new classifier layers.\n",
    "#2. Phase 2 (Fine-Tuning): Unfreeze the last 50 layers of the base model and fine-tune with a lower learning rate.\n",
    "\n",
    "# Load the MobileNetV2 model without the top classifier layers\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(target_height, target_width, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Phase 1: Train with Frozen Base\n",
    "base_model.trainable = False\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with a higher learning rate for initial convergence\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n--- Training Phase 1 (Frozen Base) ---\")\n",
    "history_phase1 = model.fit(train_ds, epochs=20, validation_data=val_ds, verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Phase 2: Fine-Tuning\n",
    "# Unfreeze the last 50 layers of the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n--- Training Phase 2 (Fine-Tuning) ---\")\n",
    "history_phase2 = model.fit(train_ds, epochs=10, validation_data=val_ds, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-03-08 23:08:57.726708: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step\n",
      "\n",
      "Validation Confusion Matrix:\n",
      "[[11  3]\n",
      " [ 0 14]]\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " NormalFinal       1.00      0.79      0.88        14\n",
      "   ScolFinal       0.82      1.00      0.90        14\n",
      "\n",
      "    accuracy                           0.89        28\n",
      "   macro avg       0.91      0.89      0.89        28\n",
      "weighted avg       0.91      0.89      0.89        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Model Evaluation on Validation Set\n",
    "\n",
    "# After training, we evaluate the model using the validation dataset by generating a confusion matrix and classification report.\n",
    "# Save the trained model\n",
    "model.save('spine_model.h5')\n",
    "\n",
    "# Predict on validation dataset\n",
    "y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "y_pred = np.argmax(model.predict(val_ds), axis=1)\n",
    "\n",
    "print(\"\\nValidation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nValidation Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion\n",
    "\n",
    "# In this training notebook, we:\n",
    "# Loaded and preprocessed the dataset with augmentation.\n",
    "# Built and trained a model using MobileNetV2 with a two-phase training approach.\n",
    "# Evaluated the model using detailed metrics on the validation set.\n",
    "\n",
    "#The final model (`spine_model.h5`) is now saved and ready for further testing or deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this training notebook, we:\n",
    "## - Loaded and preprocessed the dataset with augmentation.\n",
    "## - Built and trained a model using MobileNetV2 with a two-phase training approach.\n",
    "## - Evaluated the model using detailed metrics on the validation set.\n",
    "\n",
    "# - The final model (`spine_model.h5`) is now saved and ready for further testing or deployment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
